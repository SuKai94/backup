##scrapy爬取豆瓣小组（一）

`scrapy`是python的开源爬虫框架。以前写过基于过程思想+`bs4`的爬虫，最近学习了scrapy框架，并对[豆瓣小组](http://www.douban.com/group/explore)页面的值得加入的小组进行爬取。

<!--more-->

scrapy的入门还算简单，没什么门槛。爬取最关键的是正确的选择器和数据处理。

###1.定义`item`
```python
# coding: utf-8
from scrapy.item import Item, Field

class DoubanItem(Item):
    # define the fields for your item here like:
    # name = Field()
    groupname = Field()
    groupurl = Field()
    totalnumber = Field()
```

###2.实现`spider`
```python
# coding: utf-8
from scrapy.spider import BaseSpider
from scrapy.selector import HtmlXPathSelector
from scrapy.item import Item
from douban.items import DoubanItem

class GroupTestSpider(BaseSpider):
    name = "doubanxz"
	allowed_domains = ["douban.com"]
	start_urls = [
		"http://www.douban.com/group/explore"
	]

	def parse(self, response):
		hxs = HtmlXPathSelector(response)
		sites = hxs.select("//li[@class='']/div[@class='info']")
		items = []
		for site in sites:
			item = DoubanItem()
			item['groupname'] = site.select("div[@class='title']/a[@class='']/text()").extract()
			item['groupurl'] = site.select("div[@class='title']/a[@class='']/@href").extract()
			item['totalnumber'] = site.select("span[@class='num']/text()").extract()
			result = ''.join(repr(item).decode("unicode-escape")).encode('utf-8')
			with open('result.txt', 'a') as code:
				code.write(result+'\n')
			items.append(item)
			# print repr(item).decode("unicode-escape") + '\n'
		return items
```
说明：在spider类里面我将爬去的数据写入了result.txt文件，因为我使用feed export写入时解决不了编码问题。所以，我用了这个笨而又实际方法。

###3.运行你的蜘蛛吧
命令行输入（注意cd到项目的根目录，否则失败）:

```bash
scrapy crawl doubanxz 
```
看看根目录下的result.txt内容，是不是爬到了？

###备注：
想要让爬去的结果输入到json文件里，有两种方法解决：

1. 命令行输入：
```bash
scrapy crawl doubanxz -o result.json -t json
```
此时会出现result.json文件，里面有爬去的数据

2. 在`pipelines`文件里自己定义管道文件，顺便可以[解决编码问题](http://www.addbook.cn/blog/scrapy%E4%B8%AD%E6%96%87%E7%BC%96%E7%A0%81%E9%97%AE%E9%A2%98)。此时，运行蜘蛛只要输入即可:
```bash
scrapy crawl doubanxz 
```



